{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = '../preped.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Release Year'] = pd.to_datetime(df['Release Date'],unit='s').dt.year\n",
    "df['Release Month'] = pd.to_datetime(df['Release Date'],unit='s').dt.month\n",
    "df['Release Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age groups\n",
    "def age_group(age):\n",
    "    if age == 0:\n",
    "        return 'All'\n",
    "    elif 1 <= age <= 16:\n",
    "        return 'Teen'\n",
    "    else:\n",
    "        return 'Adult'\n",
    "df['Age Group'] = df['Minimum Age'].apply(age_group)\n",
    "\n",
    "df['Age Group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the feature columns and target column\n",
    "features = df.drop(columns=['Minimum Age', 'Age Group']).select_dtypes(include=[int, float])\n",
    "target_column = 'Age Group'\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = features\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create heatmap of confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Adult', 'All', 'Teen'],\n",
    "            yticklabels=['Adult', 'All', 'Teen'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(features.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(features.shape[1]), [features.columns[i] for i in indices], rotation=90)\n",
    "plt.xlim([-1, features.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Important Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Identify unimportant features based on a threshold (e.g., importance < 0.01)\n",
    "important_features_indices = indices[importances[indices] > 0.01]\n",
    "important_features = features.columns[important_features_indices]\n",
    "\n",
    "# Subset the features DataFrame to include only important features\n",
    "X_important = features[important_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_important, X_test_important, y_train, y_test = train_test_split(X_important, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest model with important features\n",
    "rf_model_important = RandomForestClassifier(random_state=42)\n",
    "rf_model_important.fit(X_train_important, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_important = rf_model_important.predict(X_test_important)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy (Important Features Only):\", accuracy_score(y_test, y_pred_important))\n",
    "print(classification_report(y_test, y_pred_important))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = rf_model_important.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importances (Important Features Only)\")\n",
    "plt.bar(range(X_important.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_important.shape[1]), [important_features[i] for i in indices], rotation=90)\n",
    "plt.xlim([-1, X_important.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize using HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ... (Load data and split into X_train, X_test, y_train, y_test as before)\n",
    "\n",
    "# Define the parameter grid *without* n_estimators\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],      \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4]      \n",
    "}\n",
    "\n",
    "# Define the range of n_estimators to try\n",
    "n_estimators_range = [50, 100, 200, 300]  # Expanded range\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_n_estimators = None\n",
    "best_params = None\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42) # Fixed n_estimators for this inner loop\n",
    "\n",
    "    n_train_samples = X_train.shape[0]  # Get the number of training samples\n",
    "\n",
    "    halving_cv = HalvingGridSearchCV(\n",
    "        rf_model, \n",
    "        param_grid, \n",
    "        cv=5,                \n",
    "        resource='n_samples',  # Use n_samples as resource now!\n",
    "        max_resources=int(n_train_samples * 0.5),      # 50% of training data as int\n",
    "        factor=2,             \n",
    "        min_resources=int(n_train_samples * 0.1),       # 10% of training data as int\n",
    "        scoring='accuracy',   \n",
    "        n_jobs=-1,           \n",
    "        verbose=0             # Keep it quiet inside the loop\n",
    "    )\n",
    "\n",
    "    halving_cv.fit(X_train, y_train)\n",
    "\n",
    "    best_inner_model = halving_cv.best_estimator_\n",
    "    y_pred = best_inner_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = best_inner_model\n",
    "        best_n_estimators = n_estimators\n",
    "        best_params = halving_cv.best_params_\n",
    "\n",
    "print(\"Best n_estimators:\", best_n_estimators)\n",
    "print(\"Best hyperparameters (other):\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(classification_report(y_test, best_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imoprtant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(features.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(features.shape[1]), [features.columns[i] for i in indices], rotation=90)\n",
    "plt.xlim([-1, features.shape[1]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
