{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = '../../preped.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age groups\n",
    "def age_group(age):\n",
    "    if age == 0:\n",
    "        return 'All'\n",
    "    elif 1 <= age <= 16:\n",
    "        return 'Teen'\n",
    "    else:\n",
    "        return 'Adult'\n",
    "df['Age Group'] = df['Minimum Age'].apply(age_group)\n",
    "\n",
    "df['Age Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features\n",
    "features = ['Hidden Gem Score', 'Runtime', 'Awards Received', 'Awards Nominated For',\n",
    "            'Boxoffice', 'IMDb Votes', 'Minimum Age'] + \\\n",
    "            [col for col in df.columns if col in ['Action', 'Adventure', 'Animation', \n",
    "            'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', \n",
    "            'Fantasy', 'History', 'Horror', 'Music', 'Musical', 'Mystery', \n",
    "            'News', 'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War', 'Western']]\n",
    "\n",
    "# Define target columns\n",
    "targets = ['IMDb Score', 'Rotten Tomatoes Score', 'Metacritic Score']\n",
    "\n",
    "X = df[features]\n",
    "y = df[targets]  # y now contains all target variables\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Model Creation (Adjust output layer to match the number of targets)\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(len(targets))  # Output layer with a neuron for each target\n",
    "])\n",
    "\n",
    "# Model Compilation (Use appropriate loss for multiple targets)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse']) # MSE is suitable for multiple regression targets\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Model Evaluation\n",
    "loss, *metrics = model.evaluate(X_test, y_test, verbose=0) # *metrics unpacks MAE and MSE for each target\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for each target\n",
    "for i, target_name in enumerate(targets):\n",
    "    r2 = r2_score(y_test.iloc[:, i], y_pred[:, i]) # Compare corresponding columns\n",
    "    print(f\"R-squared ({target_name}): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Error Fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training history (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss (Error)')\n",
    "\n",
    "# Plot MAE (for the first target - you can add more plots for other targets)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('Mean Absolute Error')\n",
    "\n",
    "plt.tight_layout() # Adjust subplot params so that subplots fit in to the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract predictions for each target\n",
    "y_pred_im = y_pred[:, 0]\n",
    "y_pred_rt = y_pred[:, 1]\n",
    "y_pred_mc = y_pred[:, 2]\n",
    "\n",
    "# Extract actual values for each target\n",
    "y_test_im = y_test['IMDb Score']\n",
    "y_test_rt = y_test['Rotten Tomatoes Score']\n",
    "y_test_mc = y_test['Metacritic Score']\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot IMDb scores\n",
    "ax1.scatter(y_test_im, y_pred_im, alpha=0.5)\n",
    "ax1.plot([y_test_im.min(), y_test_im.max()], [y_test_im.min(), y_test_im.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual IMDb Score')\n",
    "ax1.set_ylabel('Predicted IMDb Score')\n",
    "ax1.set_title('IMDb Score: Actual vs Predicted')\n",
    "\n",
    "# Plot Rotten Tomatoes scores\n",
    "ax2.scatter(y_test_rt, y_pred_rt, alpha=0.5)\n",
    "ax2.plot([y_test_rt.min(), y_test_rt.max()], [y_test_rt.min(), y_test_rt.max()], 'r--', lw=2)\n",
    "ax2.set_xlabel('Actual Rotten Tomatoes Score')\n",
    "ax2.set_ylabel('Predicted Rotten Tomatoes Score')\n",
    "ax2.set_title('Rotten Tomatoes: Actual vs Predicted')\n",
    "\n",
    "# Plot Metacritic scores\n",
    "ax3.scatter(y_test_mc, y_pred_mc, alpha=0.5)\n",
    "ax3.plot([y_test_mc.min(), y_test_mc.max()], [y_test_mc.min(), y_test_mc.max()], 'r--', lw=2)\n",
    "ax3.set_xlabel('Actual Metacritic Score')\n",
    "ax3.set_ylabel('Predicted Metacritic Score')\n",
    "ax3.set_title('Metacritic: Actual vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize using HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor  # Important!\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Select features\n",
    "features = ['Hidden Gem Score', 'Runtime', 'Awards Received', 'Awards Nominated For',\n",
    "            'Boxoffice', 'IMDb Votes', 'Minimum Age'] + \\\n",
    "            [col for col in df.columns if col in ['Action', 'Adventure', 'Animation', \n",
    "            'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', \n",
    "            'Fantasy', 'History', 'Horror', 'Music', 'Musical', 'Mystery', \n",
    "            'News', 'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War', 'Western']]\n",
    "\n",
    "# Define target columns\n",
    "targets = ['IMDb Score', 'Rotten Tomatoes Score', 'Metacritic Score']\n",
    "\n",
    "X = df[features]\n",
    "y = df[targets]  # y now contains all target variables\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# 1. Create a function that builds your Keras model:\n",
    "def build_model(units1=64, units2=32, learning_rate=0.001):  # Add hyperparameters as arguments\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(units1, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dropout(0.2), # Example dropout\n",
    "        layers.Dense(units2, activation='relu'),\n",
    "        layers.Dense(len(targets))\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)  # Use the passed learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# 2. Wrap your Keras model in a KerasRegressor:\n",
    "keras_reg = KerasRegressor(build_fn=build_model)\n",
    "\n",
    "# 3. Define your hyperparameter grid:\n",
    "param_grid = {\n",
    "    'units1': [32, 64, 128],  # Number of neurons in the first layer\n",
    "    'units2': [16, 32, 64],   # Number of neurons in the second layer\n",
    "    'learning_rate': [0.001, 0.0001, 0.01], # Learning Rate\n",
    "    'epochs': [50, 100, 200],  # Number of epochs (HalvingGridSearchCV will handle early stopping)\n",
    "    'batch_size': [16, 32, 64]   # Batch size\n",
    "}\n",
    "\n",
    "# 4. Create the HalvingGridSearchCV object:\n",
    "halving_cv = HalvingGridSearchCV(\n",
    "    keras_reg,\n",
    "    param_grid,\n",
    "    cv=3,  # Number of cross-validation folds\n",
    "    factor=3,  # Reduction factor (how many combinations are kept in each iteration)\n",
    "    max_resources=200, # Maximum epochs to train for\n",
    "    min_resources=50, # Minimum epochs to train for\n",
    "    scoring='neg_mean_squared_error',  # Scoring metric (important for regression)\n",
    "    verbose=1,  # For progress updates\n",
    "    n_jobs=-1 # Use all available processors\n",
    ")\n",
    "\n",
    "# 5. Fit the HalvingGridSearchCV object:\n",
    "halving_cv.fit(X_train, y_train)\n",
    "\n",
    "# 6. Get the best model and hyperparameters:\n",
    "best_model = halving_cv.best_estimator_\n",
    "best_params = halving_cv.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# 7. Evaluate the best model on the test set:\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics as before\n",
    "loss, *metrics = best_model.model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "num_targets = len(targets)\n",
    "for i in range(num_targets):\n",
    "    mae = metrics[i * 2]\n",
    "    mse = metrics[i * 2 + 1]\n",
    "    print(f\"Test MAE ({targets[i]}): {mae}\")\n",
    "    print(f\"Test MSE ({targets[i]}): {mse}\")\n",
    "\n",
    "for i, target_name in enumerate(targets):\n",
    "    r2 = r2_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    print(f\"R-squared ({target_name}): {r2}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for each target\n",
    "for i, target_name in enumerate(targets):\n",
    "    r2 = r2_score(y_test.iloc[:, i], y_pred[:, i]) # Compare corresponding columns\n",
    "    print(f\"R-squared ({target_name}): {r2}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
